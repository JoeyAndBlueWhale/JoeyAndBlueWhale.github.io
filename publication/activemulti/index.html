<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: August 10, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d1074f8af93a3c818e7ddf864bccad0a.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Yihan Zhou (Joey)" />





  

<meta name="description" content="Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of {{&lt; math &gt;}}$k${{&lt; /math &gt;}} distributions, {{&lt; math &gt;}}$\{D_i\}_{i\in[k]}${{&lt; /math &gt;}}, is considered and a classifier’s performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.&lt;/br&gt;&lt;/br&gt;In this paper, we develop new algorithms for active multi-distribution learning and establish improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings. Specifically, in the near-realizable setting we prove an upper bound of {{&lt; math &gt;}}$\widetilde{O}\Bigl(\theta_{\max}(d&#43;k)\ln\frac{1}{\epsilon}\Bigr)${{&lt; /math &gt;}} and {{&lt; math &gt;}}$\widetilde{O}\Bigl(\theta_{\max}(d&#43;k)\Bigl(\ln\frac{1}{\epsilon}&#43;\frac{\nu^2}{\varepsilon^2}\Bigr)&#43;\frac{k\nu}{\varepsilon^2}\Bigr)${{&lt; /math &gt;}} in the realizable and agnostic settings respectively, where {{&lt; math &gt;}}$\theta_{\max}${{&lt; /math &gt;}} is the maximum disagreement coefficient among the {{&lt; math &gt;}}$k${{&lt; /math &gt;}} distributions, {{&lt; math &gt;}}$d${{&lt; /math &gt;}} is the VC dimension of the hypothesis class, {{&lt; math &gt;}}$\nu${{&lt; /math &gt;}} is the multi-distribution error of the best hypothesis, and {{&lt; math &gt;}}$\varepsilon${{&lt; /math &gt;}} is the target excess error. Moreover, we show that the bound in the realizable setting is information-theoretically optimal and that the {{&lt; math &gt;}}$\frac{k\nu}{\varepsilon^2}${{&lt; /math &gt;}} term in the agnostic setting is fundamental for proper learners. We also establish instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes (Blum et al., 2017; Zhang et al., 2024), which may be of independent interest." />



<link rel="alternate" hreflang="en-us" href="https://joeyandbluewhale.github.io/publication/activemulti/" />
<link rel="canonical" href="https://joeyandbluewhale.github.io/publication/activemulti/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hua235b20c26de23a006d4e436991121ab_1867_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hua235b20c26de23a006d4e436991121ab_1867_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://joeyandbluewhale.github.io/media/icon_hua235b20c26de23a006d4e436991121ab_1867_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Joey&#39;s Homepage" />
<meta property="og:url" content="https://joeyandbluewhale.github.io/publication/activemulti/" />
<meta property="og:title" content="Towards Fundamental Limits for Active Multi-distribution Learning (COLT 2025) | Joey&#39;s Homepage" />
<meta property="og:description" content="Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of {{&lt; math &gt;}}$k${{&lt; /math &gt;}} distributions, {{&lt; math &gt;}}$\{D_i\}_{i\in[k]}${{&lt; /math &gt;}}, is considered and a classifier’s performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.&lt;/br&gt;&lt;/br&gt;In this paper, we develop new algorithms for active multi-distribution learning and establish improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings. Specifically, in the near-realizable setting we prove an upper bound of {{&lt; math &gt;}}$\widetilde{O}\Bigl(\theta_{\max}(d&#43;k)\ln\frac{1}{\epsilon}\Bigr)${{&lt; /math &gt;}} and {{&lt; math &gt;}}$\widetilde{O}\Bigl(\theta_{\max}(d&#43;k)\Bigl(\ln\frac{1}{\epsilon}&#43;\frac{\nu^2}{\varepsilon^2}\Bigr)&#43;\frac{k\nu}{\varepsilon^2}\Bigr)${{&lt; /math &gt;}} in the realizable and agnostic settings respectively, where {{&lt; math &gt;}}$\theta_{\max}${{&lt; /math &gt;}} is the maximum disagreement coefficient among the {{&lt; math &gt;}}$k${{&lt; /math &gt;}} distributions, {{&lt; math &gt;}}$d${{&lt; /math &gt;}} is the VC dimension of the hypothesis class, {{&lt; math &gt;}}$\nu${{&lt; /math &gt;}} is the multi-distribution error of the best hypothesis, and {{&lt; math &gt;}}$\varepsilon${{&lt; /math &gt;}} is the target excess error. Moreover, we show that the bound in the realizable setting is information-theoretically optimal and that the {{&lt; math &gt;}}$\frac{k\nu}{\varepsilon^2}${{&lt; /math &gt;}} term in the agnostic setting is fundamental for proper learners. We also establish instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes (Blum et al., 2017; Zhang et al., 2024), which may be of independent interest." /><meta property="og:image" content="https://joeyandbluewhale.github.io/media/icon_hua235b20c26de23a006d4e436991121ab_1867_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-06-28T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2025-06-28T00:00:00&#43;00:00">
  






    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://joeyandbluewhale.github.io/publication/activemulti/"
  },
  "headline": "Towards Fundamental Limits for Active Multi-distribution Learning (COLT 2025)",
  
  "datePublished": "2025-06-28T00:00:00Z",
  "dateModified": "2025-06-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Chicheng Zhang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Joey's Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "https://joeyandbluewhale.github.io/media/icon_hua235b20c26de23a006d4e436991121ab_1867_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of {{\u003c math \u003e}}$k${{\u003c /math \u003e}} distributions, {{\u003c math \u003e}}$\\{D_i\\}_{i\\in[k]}${{\u003c /math \u003e}}, is considered and a classifier’s performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.\u003c/br\u003e\u003c/br\u003eIn this paper, we develop new algorithms for active multi-distribution learning and establish improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings. Specifically, in the near-realizable setting we prove an upper bound of {{\u003c math \u003e}}$\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\ln\\frac{1}{\\epsilon}\\Bigr)${{\u003c /math \u003e}} and {{\u003c math \u003e}}$\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\Bigl(\\ln\\frac{1}{\\epsilon}+\\frac{\\nu^2}{\\varepsilon^2}\\Bigr)+\\frac{k\\nu}{\\varepsilon^2}\\Bigr)${{\u003c /math \u003e}} in the realizable and agnostic settings respectively, where {{\u003c math \u003e}}$\\theta_{\\max}${{\u003c /math \u003e}} is the maximum disagreement coefficient among the {{\u003c math \u003e}}$k${{\u003c /math \u003e}} distributions, {{\u003c math \u003e}}$d${{\u003c /math \u003e}} is the VC dimension of the hypothesis class, {{\u003c math \u003e}}$\\nu${{\u003c /math \u003e}} is the multi-distribution error of the best hypothesis, and {{\u003c math \u003e}}$\\varepsilon${{\u003c /math \u003e}} is the target excess error. Moreover, we show that the bound in the realizable setting is information-theoretically optimal and that the {{\u003c math \u003e}}$\\frac{k\\nu}{\\varepsilon^2}${{\u003c /math \u003e}} term in the agnostic setting is fundamental for proper learners. We also establish instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes (Blum et al., 2017; Zhang et al., 2024), which may be of independent interest."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Towards Fundamental Limits for Active Multi-distribution Learning (COLT 2025) | Joey&#39;s Homepage</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="a7b6c38b313ed871001010579aaade45" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  




  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Bio</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Research</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Working Experiences</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    








<div class="pub">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>Towards Fundamental Limits for Active Multi-distribution Learning (COLT 2025)</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Chicheng Zhang</span>, <span >
      Yihan Zhou</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June, 2025
  </span>
  

  

  

  
  
  
  

  
  

</div>

    




<div class="btn-links mb-3">
  
  








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/abs/2506.17607" target="_blank" rel="noopener">
    arxiv</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header" href="/files/colt2025.mp4" >
    15min talk</a>


</div>


  
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of 

$k$ distributions, 

$\{D_i\}_{i\in[k]}$, is considered and a classifier’s performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.</br></br>In this paper, we develop new algorithms for active multi-distribution learning and establish improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings. Specifically, in the near-realizable setting we prove an upper bound of 

$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\epsilon}\Bigr)$ and 

$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\epsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$ in the realizable and agnostic settings respectively, where 

$\theta_{\max}$ is the maximum disagreement coefficient among the 

$k$ distributions, 

$d$ is the VC dimension of the hypothesis class, 

$\nu$ is the multi-distribution error of the best hypothesis, and 

$\varepsilon$ is the target excess error. Moreover, we show that the bound in the realizable setting is information-theoretically optimal and that the 

$\frac{k\nu}{\varepsilon^2}$ term in the agnostic setting is fundamental for proper learners. We also establish instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes (Blum et al., 2017; Zhang et al., 2024), which may be of independent interest.</p>
    

    
    

    

    <div class="space-below"></div>

    <div class="article-style"></div>

    







<div class="share-box">
  <ul class="share">
    
  </ul>
</div>











  
  
    




  
    




  
















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js"></script>




  

  
  

  








































<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.387a7b38a3dfead6f96c96742d20f5af.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
