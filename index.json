[{"authors":["admin"],"categories":null,"content":"I just finished my Master program at UBC. I was fortunate enough to be co-supervised by Nick Harvey and Mark Schmidt. During my Master\u0026rsquo;s studies, my research revolved around first-order optimization methods and online learning. I am interested in statistical learning theory and have started to learn privacy and fairness in machine learning recently. In addition, I\u0026rsquo;m interested general theoretical computer science.\nI enjoy poems, movies and literature. García Márquez is my favorite writer. I watch soccer, basketball and Dota 2. Liverpool and Celtics are my home teams.\n\u0026ldquo;I\u0026rsquo;m one with the force and the force is with me.\u0026rdquo; \u0026ndash; Chirrut Îmwe in Rogue One\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1608006474,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://JoeyAndBlueWhale.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I just finished my Master program at UBC. I was fortunate enough to be co-supervised by Nick Harvey and Mark Schmidt. During my Master\u0026rsquo;s studies, my research revolved around first-order optimization methods and online learning. I am interested in statistical learning theory and have started to learn privacy and fairness in machine learning recently. In addition, I\u0026rsquo;m interested general theoretical computer science.\nI enjoy poems, movies and literature. García Márquez is my favorite writer.","tags":null,"title":"Yihan Zhou(Joey)","type":"authors"},{"authors":["Yihan Zhou, Victor Portella, Mark Schmidt, Nick Harvey"],"categories":null,"content":"","date":1598572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"09f3492501d555be372cafed0eb0dccc","permalink":"https://JoeyAndBlueWhale.github.io/publication/relativelip/","publishdate":"2020-08-28T00:00:00Z","relpermalink":"/publication/relativelip/","section":"publication","summary":"Online convex optimization (OCO) is a powerful algorithmic framework that has extensive applications in different areas. Regret is a commonly-used measurement for the performance of algorithms in this framework. Lipschitz continuity of the cost functions is commonly assumed in order to obtain sublinear regret, that is to say, this condition is usually necessary for theoretical guarantees for good performances of OCO algorithms. Moreover, strong convexity of cost functions can sometimes give even better theoretical performance bounds, more specifically, logarithmic regret. Recently, researchers from convex optimization proposed the notions of \"relative Lipschitz continuity\" and \"relative strong convexity\". Both of the notions are generalizations of their classical counterparts. It has been shown that subgradient methods in the relative setting have performance analogous to their performance in the classical setting.\nIn this work, we consider OCO for relative Lipschitz and relative strongly convex functions. We extend the known regret bounds for classical OCO algorithms to the relative setting. Specifically, we show regret bounds for the follow the regularized leader algorithms and a variant of online mirror descent. Due to the generality of these methods, these results yield regret bounds for a wide variety of OCO algorithms. Furthermore, we extend the results to algorithms with extra regularization such as regularized dual averaging.","tags":null,"title":"Regret Bounds without Lipschitz Continuity: Online Learning with Relative-Lipschitz Losses (NeurIPS 2020)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1596632400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"ea04c2ff110cc7b69c7a75ce5223953c","permalink":"https://JoeyAndBlueWhale.github.io/talk/samplecomplexitylqr/","publishdate":"2020-08-05T13:00:00Z","relpermalink":"/talk/samplecomplexitylqr/","section":"talk","summary":"A talk about the sample complexity of the Linear Quadratic Regulator.","tags":null,"title":"Sample Complexity of the Linear Quadratic Regulator","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"faeeb3c1c0e64f6872de69ea735bfde3","permalink":"https://JoeyAndBlueWhale.github.io/publication/coordinatedescent/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/publication/coordinatedescent/","section":"publication","summary":"Work in progress.","tags":null,"title":"Improved Analyses of Block-Coordinate Descent for Linearly-Constrained, Composite Objectives","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"d318660e444b63c5ea06f780e9fc4447","permalink":"https://JoeyAndBlueWhale.github.io/publication/antidepressant/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/publication/antidepressant/","section":"publication","summary":"Submitted to PLOS One, under review.","tags":null,"title":"Replication of Machine-Learning Analyses to Predict Response to Antidepressant Medications in Patients with Major Depressive Disorder","type":"publication"},{"authors":null,"categories":null,"content":"","date":1574254800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"908dbb2e469589340afbf02db92c37ed","permalink":"https://JoeyAndBlueWhale.github.io/talk/kernellearning/","publishdate":"2019-11-20T13:00:00Z","relpermalink":"/talk/kernellearning/","section":"talk","summary":"Similar to deep neural networks, kernel machines have good generalization behaviour. However, such behaviour cannot be explained by existing theoretical results.","tags":null,"title":"Generalization of Kernel Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1560963600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"3c77ac88652feb6f7d6de10cc2c1ae81","permalink":"https://JoeyAndBlueWhale.github.io/talk/onlinelearning/","publishdate":"2019-06-19T17:00:00Z","relpermalink":"/talk/onlinelearning/","section":"talk","summary":"An overview of online learning and different bandit problem.","tags":null,"title":"Online Learning and Bandits","type":"talk"},{"authors":null,"categories":null,"content":"","date":1555347600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"a3e6f3fbaa494b2666333ecea0ef84cd","permalink":"https://JoeyAndBlueWhale.github.io/talk/dictionarylearning/","publishdate":"2019-04-15T17:00:00Z","relpermalink":"/talk/dictionarylearning/","section":"talk","summary":"An introduction to dictionary learning and its applications.","tags":null,"title":"Dictionary Learning","type":"talk"},{"authors":null,"categories":null,"content":"This is a part time URA(Undergrad Research Assistant) I did with Professor Peter Van Beek. White balance is a vital step in image processing. Due to computation limitations, current white balancing algorithms for cameras are simple. But if we ignore this restriction and apply ML algorithms, state-of-arts performance can be achieved. We explored different machine learning/deep learning methods to improve white balancing performance for photo processing.\nI implemented some popular data augmentation tricks and domain-adaptation heuristics in MATLAB on existing algorithm and improved the performance slightly.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"1bd7397e7f8a629fc99b303b6888e653","permalink":"https://JoeyAndBlueWhale.github.io/project/whitebalance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/whitebalance/","section":"project","summary":"Developing ML algorithms for white balancing in order to achieve better performance.","tags":["undergrad"],"title":"Machine Learning Algorithms for White Balancing","type":"project"},{"authors":null,"categories":null,"content":"We conduct a survey on privacy in machine learning. First we show examples of how people could use machine learning to invade privacy and how machine learning could cause some unprecedented privacy issues. Then we give the rigorous definition of privacy in mathematics. In the end of this paper, we demonstrate three specific privacy preserving machine learning algorithms and one noise generation method.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"e01ad18e2feb8dffa0934aa55cb4da80","permalink":"https://JoeyAndBlueWhale.github.io/project/mlprivacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/mlprivacy/","section":"project","summary":"A survey on privacy in machine learning.","tags":["undergrad"],"title":"Privacy in Machine Learning","type":"project"},{"authors":null,"categories":null,"content":"The query complexity of the learner in the presence of an adversary is investigated in Tsitsiklis et al. In this work, it is assumed that the adversary gets to observe all the learners queries but is oblivious to the responses that the learner gets for each query. Tsitsiklis et al also introduces the same problem under the bayesian setting and provides bounds on the query complexity when operating under a uniform prior over the queried values. We provide tighter bounds for this case thus managing to improve both the upper and lower bound on the query complexity. For the upper bound, we propose a new algorithm that is better than replicated bisection search under certain assumptions of the value of parameters. In addition we improve the lower bound with an additive factor. We also investigate the private query model in higher dimension, which is mentioned as one of the open problems in Tsitsiklis et al. We successfully extend the opportunistic bisection strategy to higher dimension and derive an upper bound based on the strategy. We use the same method as in Tsitsiklis et al to prove the lower bound with a strengthened accuracy constraint and derive a less tighter lower bound with the original accuracy constraint.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"8c15ab18e7a0220796196a1fc8f7cb6d","permalink":"https://JoeyAndBlueWhale.github.io/project/privatesequentiallearning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/privatesequentiallearning/","section":"project","summary":"Extending current theoretical results on private sequential learning to Bayesian setting and higher dimensions.","tags":["master"],"title":"Private Sequential Learning","type":"project"},{"authors":null,"categories":null,"content":"This is a course project led by a medical student. We replicated a study using ML algorithms for the prediction of antidepressant response on a new dataset and tested various new ML models on this task. I implemented all the ML models. We have already submitted the paper to a medical journal. The project report is not published here due to confidentiality of the medical dataset used.\nMajor depressive disorder (MDD) is the second-leading cause of disability globally. This depressive disorder is marked by low mood, anhedonia (inability to experience joy) and neurovegetative symptoms such as low energy, poor concentration, and over- or under-sleeping. The condition can affects all aspects of life, from hampering or even preventing work, to increasing mortality from other medical conditions. Understandably, depression is a common reason to access healthcare, and is associated with significant healthcare costs.\nThe mainstay and first-line therapy physicians use for MDD remains prescribing an antidepressant, a variety of medications of different mechanisms. While effective, only a little over half of patients will respond to this initial therapy, and many will need to try a different agent. Many options are available for patients who do not respond to initial treatment with an antidepressant, though it can takes months of trial and error until these options are attempted.\nThis presents a promising application for predictive modelling. If a patient can be predicted to have a high chance of not responding to treatment with an antidepressant, treatment outcomes may be improved by giving these patients more aggressive therapy sooner. For example, a high-risk patient might benefit from having a second, adjunctive medication right away, or may be prioritized to also engage in psychotherapy (counselling). Both have been shown to increase depression response rates, but have barriers to be used widely such as costs or side-effects.\nOur projects seeks to continue this work improving the prediction of antidepressant response from clinical data. We do this in three ways: creating a reproducible, transparent, and reusable automated method for data processing, replicating the recent work by Nie et al. and externally validating their model on a new dataset, and by testing new, previously unpublished methods for both prediction and feature selection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"db7fbba85a5e6740849d3542ca7f94b0","permalink":"https://JoeyAndBlueWhale.github.io/project/antidep/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/antidep/","section":"project","summary":"Replicating a study using ML algorithms for the prediction of antidepressant response on a different dataset and attempting to improve the performance.","tags":["master"],"title":"Replicating and Improving the Prediction of Antidepressant Response Using the CAN-BIND and STAR*D Datasets","type":"project"},{"authors":null,"categories":null,"content":"This is a part time URA (Undergrad Research Assistant) supervised by Yaoliang Yu. Frank-Wolfe algorithm is a first-order optimization algorithm for constraint optimization problems. The per iteration cost for Frank-Wolfe algorithm is low because it doesn\u0026rsquo;t need the projection step. Recently this algorithm has gained attention again. This project involves in a study of this algorithm and its variants. We also attempted to relax the constraints of some of them.\nI simplified one algorithm and provided a simpler proof.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608006474,"objectID":"41faa152c695d5d49f756ec304378032","permalink":"https://JoeyAndBlueWhale.github.io/project/frankwolfe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/frankwolfe/","section":"project","summary":"Studying Frank-Wolfe algorithm and its variants.","tags":["undergrad"],"title":"Variants of Frank-Wolfe algorithm","type":"project"}]